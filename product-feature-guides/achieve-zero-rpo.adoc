---
permalink: product-feature-guides/achieve-zero-rpo.html 
sidebar: sidebar 
keywords: rpo, zero, rto, on-premises, storagegrid, object, replication, synchronous 
summary: 'Dieser technische Bericht beschreibt die Implementierung von StorageGRID Replizierungsstrategien zum Erreichen eines Recovery Point Objective (RPO) von 0 bei einem Standortausfall.' 
---
= RPO von null mit StorageGRID – Ein umfassender Leitfaden zur Replizierung an mehreren Standorten
:hardbreaks:
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Dieser technische Bericht enthält einen umfassenden Leitfaden zur Implementierung von StorageGRID Replizierungsstrategien zum Erreichen eines Recovery Point Objective (RPO) von null bei einem Standortausfall. Im Dokument werden verschiedene Bereitstellungsoptionen für StorageGRID beschrieben, darunter synchrone Replikation an mehreren Standorten und asynchrone Replikation in mehreren Grids. Es wird erläutert, wie die Richtlinien für Information Lifecycle Management (ILM) von StorageGRID konfiguriert werden können, um die Datenaufbewahrung und Verfügbarkeit von Daten über mehrere Standorte hinweg zu gewährleisten. Darüber hinaus werden Leistungsüberlegungen, Ausfallszenarien und Wiederherstellungsprozesse behandelt, um einen unterbrechungsfreien Clientbetrieb aufrechtzuerhalten. Ziel dieses Dokuments ist es, die Informationen bereitzustellen, die gewährleisten, dass die Daten auch bei einem vollständigen Standortausfall zugänglich und konsistent bleiben, indem sowohl synchrone als auch asynchrone Replikationstechniken genutzt werden.



== Übersicht über StorageGRID

NetApp StorageGRID ist ein objektbasiertes Storage-System, das die branchenübliche API Amazon Simple Storage Service (Amazon S3) unterstützt.

StorageGRID bietet einen Single Namespace über mehrere Standorte mit variablen Service-Leveln basierend auf Information Lifecycle Management-Richtlinien (ILM). Mit diesen Lebenszyklusrichtlinien können Sie den Speicherort Ihrer Daten während ihres gesamten Lebenszyklus optimieren.

StorageGRID ermöglicht die konfigurierbare Aufbewahrung und Verfügbarkeit Ihrer Daten in lokalen und geografisch verteilten Lösungen. Unabhängig davon, ob sich Ihre Daten vor Ort oder in einer öffentlichen Cloud befinden, ermöglichen integrierte Hybrid-Cloud-Workflows Ihrem Unternehmen die Nutzung von Cloud-Diensten wie Amazon Simple Notification Service (Amazon SNS), Google Cloud, Microsoft Azure Blob, Amazon S3 Glacier, Elasticsearch und mehr.



=== StorageGRID Scale

StorageGRID kann mit nur 3 Storage-Nodes implementiert werden und ein einzelnes Grid kann auf bis zu 200 Nodes erweitert werden. Ein einzelnes Grid kann entweder als einzelner Standort oder bis zu 16 Standorte implementiert werden. Ein minimales Grid besteht aus einem Administrator-Node und 3 Storage-Nodes an einem einzelnen Standort. Der Admin-Node enthält die Managementoberfläche, einen zentralen Punkt für die Kennzahlen und die Protokollierung, und behält die Konfiguration der StorageGRID-Komponenten bei. Der Admin-Node enthält außerdem einen integrierten Load Balancer für den S3-API-Zugriff. StorageGRID kann als rein softwarebasierte Lösung, als VMware Virtual Machine Appliances oder als speziell entwickelte Appliances implementiert werden.

Ein StorageGRID-Knoten kann wie folgt bereitgestellt werden:

* Ein reiner Metadatenknoten, der die Objektanzahl maximiert
* Ein reiner Objektspeicherknoten, der den Objektspeicherplatz maximiert
* Ein kombinierter Metadaten- und Objektspeicherknoten, der sowohl die Objektanzahl als auch den Objektspeicherplatz hinzufügt


Jeder Storage-Node kann auf eine Kapazität von mehreren Petabyte für Objekt-Storage skaliert werden, sodass es sich um einen einzigen Namespace in Hunderten Petabyte handelt. StorageGRID bietet außerdem einen integrierten Load Balancer für S3-API-Vorgänge, der als Gateway-Node bezeichnet wird.

image:zero-rpo/delivery-paths.png["Implementierungsoptionen von StorageGRID"]

StorageGRID besteht aus einer Sammlung von Nodes, die in einer Standorttopologie abgelegt werden. Ein Standort in StorageGRID kann ein eindeutiger physischer Standort sein oder sich als logisches Konstrukt an einem gemeinsam genutzten physischen Standort wie andere Standorte im Grid befinden. Ein StorageGRID-Standort sollte sich nicht über mehrere physische Standorte erstrecken. Ein Standort stellt eine gemeinsame LAN-Infrastruktur (Local Area Network) dar.



=== StorageGRID und Ausfall-Domains

StorageGRID umfasst mehrere Schichten von Ausfall-Domains, die Sie bei der Entscheidung, wie Sie Ihre Lösung planen, wie Sie Ihre Daten speichern und wo Ihre Daten gespeichert werden sollten, um das Risiko von Ausfällen zu mindern.

* Grid-Ebene – Ein Grid, das aus mehreren Standorten besteht, kann einen Standortausfall oder eine Isolierung aufweisen, und der/die zugängliche(n) Standort(e) kann/können weiterhin als Grid betrieben werden.
* Standort-Ebene – Ausfälle innerhalb eines Standorts können den Betrieb dieses Standorts beeinträchtigen, beeinträchtigen aber nicht den Rest des Grids.
* Node-Ebene – Ein Node-Ausfall hat keine Auswirkungen auf den Betrieb des Standorts.
* Festplattenebene – ein Festplattenausfall beeinträchtigt den Betrieb des Node nicht.




=== Objektdaten und Metadaten

Bei Objekt-Storage ist die Storage-Einheit ein Objekt und nicht eine Datei oder ein Block. Im Gegensatz zur Baumstruktur eines File-Systems oder Block-Storage werden die Daten im Objekt-Storage in einem flachen, unstrukturierten Layout organisiert. Objekt-Storage entkoppelt den physischen Standort der Daten von der Methode zum Speichern und Abrufen dieser Daten.

Jedes Objekt in einem objektbasierten Storage-System besteht aus zwei Teilen: Objekt-Daten und Objekt-Metadaten.

* Objektdaten stellen die tatsächlichen zugrunde liegenden Daten dar, z. B. ein Foto, ein Film oder eine Patientenakte.
* Objektmetadaten sind alle Informationen, die ein Objekt beschreiben.


StorageGRID verwendet Objektmetadaten, um die Standorte aller Objekte im Grid zu verfolgen und den Lebenszyklus eines jeden Objekts mit der Zeit zu managen.

Objektmetadaten enthalten Informationen wie die folgenden:

* Systemmetadaten, einschließlich einer eindeutigen ID für jedes Objekt, des Objektnamens, des Namens des S3-Buckets, des Mandantenkontonamens oder der ID, der logischen Größe des Objekts, des Datums und der Uhrzeit der ersten Erstellung des Objekts sowie des Datums und der Uhrzeit der letzten Änderung des Objekts.
* Der aktuelle Speicherort der einzelnen Objektreplikate oder Erasure-Coded-Fragmente.
* Alle mit dem Objekt verknüpften Schlüssel-Wert-Paare für benutzerdefinierte Benutzer-Metadaten.
* Bei S3-Objekten sind alle dem Objekt zugeordneten Objekt-Tag-Schlüssel-Wert-Paare vorhanden
* Für segmentierte Objekte und mehrteilige Objekte, Segment-IDs und Datengrößen.


Objektmetadaten sind individuell anpassbar und erweiterbar und bieten dadurch Flexibilität für die Nutzung von Applikationen. Detaillierte Informationen darüber, wie und wo StorageGRID Objektmetadaten speichert, finden Sie unter https://docs.netapp.com/us-en/storagegrid/admin/managing-object-metadata-storage.html["Management von Objekt-Metadaten-Storage"].

Das Information Lifecycle Management-System (ILM) von StorageGRID wird zur Orchestrierung der Platzierung, Dauer und Aufnahme aller Objektdaten in Ihrem StorageGRID System verwendet. ILM-Regeln bestimmen, wie StorageGRID Objekte mithilfe von Replikaten der Objekte oder Erasure Coding eines Objekts über Nodes und Standorte hinweg im Zeitverlauf speichert. Dieses ILM-System ist für die Konsistenz der Objektdaten in einem Grid verantwortlich.



=== Erasure Coding

StorageGRID bietet die Möglichkeit, Code-Daten auf mehreren Ebenen zu löschen. Bei StorageGRID Appliances löschen wir die Daten, die auf jedem Node auf allen Laufwerken gespeichert sind, mit RAID. So schützen wir sie vor dem Ausfall mehrerer Festplatten, was zu Datenverlust und Unterbrechungen führt. StorageGRID kann zudem Erasure Coding-Schemata verwenden, um Objektdaten über die Nodes innerhalb eines Standorts zu speichern oder mithilfe der ILM-Regeln von StorageGRID auf 3 oder mehr Standorte im StorageGRID System zu verteilen.

Erasure Coding ermöglicht ein Storage-Layout, das resistent gegen Node-Ausfälle mit geringem Overhead ist. Die Replizierung kann dasselbe mit mehr Overhead erreichen. Alle StorageGRID Erasure Coding-Schemata können an einem einzigen Standort implementiert werden, sofern die Mindestanzahl an Nodes für die Speicherung der Datenblöcke erreicht ist. Das bedeutet, dass für ein EC-Schema von 4+2 mindestens 6 Knoten zur Verfügung stehen müssen, um die Daten zu empfangen.

image:zero-rpo/ec-schemes.png["Verfügbare StorageGRID Erasure Coding-Schemata für Objekte"]



=== Metadatenkonsistenz

In StorageGRID werden Metadaten normalerweise mit drei Replikaten pro Standort gespeichert, um Konsistenz und Verfügbarkeit zu gewährleisten. Diese Redundanz trägt dazu bei, die Datenintegrität und -Verfügbarkeit auch bei einem Ausfall aufrechtzuerhalten.

Die Standardkonsistenz wird auf einer Grid-weiten Ebene definiert. Benutzer können die Konsistenz auf Bucket-Ebene jederzeit ändern.

Die in StorageGRID verfügbaren Bucket-Konsistenzoptionen sind:

* *All*: Bietet die höchste Konsistenz. Alle Nodes im Grid erhalten die Daten sofort, andernfalls schlägt die Anforderung fehl.
* *Strong-global*: Garantiert Lese-nach-Schreiben-Konsistenz für alle Client-Anfragen über alle Standorte hinweg.
* *Strong-global V2*: Garantiert Lese-nach-Schreiben-Konsistenz für alle Client-Anfragen über alle Standorte hinweg. Gewährleistet Konsistenz für mehrere Knoten oder sogar einen Standortausfall, wenn das Quorum aus Metadaten-Replikaten erreichbar ist. Beispielsweise müssen mindestens 5 Replikate aus einem Raster mit 3 Standorten mit maximal 3 Replikaten innerhalb eines Standorts erstellt werden.
* *Strong-site*: Garantiert Lese-nach-Schreiben Konsistenz für alle Client-Anfragen innerhalb einer Site.
* *Read-after-New-write*(default): Bietet Read-after-write-Konsistenz für neue Objekte und eventuelle Konsistenz für Objektaktualisierungen. Hochverfügbarkeit und garantierte Datensicherung Empfohlen für die meisten Fälle.
* *Verfügbar*: Bietet eventuelle Konsistenz für neue Objekte und Objekt-Updates. Verwenden Sie für S3-Buckets nur nach Bedarf (z. B. für einen Bucket mit Protokollwerten, die nur selten gelesen werden, oder für HEAD- oder GET-Vorgänge für nicht vorhandene Schlüssel). Nicht unterstützt für S3 FabricPool-Buckets.




=== Konsistenz von Objektdaten

Metadaten werden automatisch innerhalb von und über Standorte hinweg repliziert, Entscheidungen zur Platzierung von Objektdaten liegen bei Ihnen. Objektdaten können in Replikaten innerhalb und über Standorte hinweg gespeichert werden, in Erasure Coding innerhalb von oder über Standorte hinweg, in einer Kombination oder in Replikaten und in Storage-Schemata, die nach Erasure Coding codiert sind. ILM-Regeln können für alle Objekte angewendet oder so gefiltert werden, dass sie nur für bestimmte Objekte, Buckets oder Mandanten gelten. ILM-Regeln legen fest, wie Objekte gespeichert werden, wie Replikate und/oder Erasure Coding codiert wird, wie lange Objekte an diesen Standorten gespeichert werden, ob sich die Anzahl der Replikate oder Erasure Coding-Schemata ändert oder sich der Standort im Laufe der Zeit ändert.

Jede ILM-Regel wird mit einem von drei Aufnahmeverhalten zum Schutz von Objekten konfiguriert: Dual Commit, Balanced oder Strict.

Die Option für die doppelte Provisionierung erstellt sofort zwei Kopien auf zwei beliebigen unterschiedlichen Storage-Nodes im Grid und gibt die Anforderung erfolgreich an den Client zurück. Die Knotenauswahl wird innerhalb des Standorts der Anforderung versucht, kann jedoch unter Umständen Knoten eines anderen Standorts verwenden. Das Objekt wird der ILM-Warteschlange hinzugefügt, die bewertet und gemäß den ILM-Regeln platziert werden soll.

Die Option „ausgeglichen“ bewertet das Objekt sofort mit der ILM-Richtlinie und platziert das Objekt synchron, bevor die Anforderung erfolgreich an den Client zurückgegeben wird. Wenn die ILM-Regel aufgrund eines Ausfalls oder aufgrund unzureichenden Storage zur Erfüllung der Platzierungsanforderungen nicht sofort erfüllt werden kann, wird stattdessen Dual Commit verwendet. Sobald das Problem behoben ist, platziert ILM das Objekt automatisch basierend auf der definierten Regel.

Die strikte Option wertet das Objekt anhand der ILM-Richtlinie sofort aus und platziert das Objekt synchron, bevor die Anforderung erfolgreich an den Client zurückgegeben wird. Wenn die ILM-Regel aufgrund eines Ausfalls oder aufgrund unzureichenden Storage nicht sofort erfüllt werden kann, um die Platzierungsanforderungen zu erfüllen, schlägt die Anforderung fehl und der Client muss einen Vorgang wiederholen.



=== Lastverteilung

StorageGRID kann mit Client-Zugriff über die integrierten Gateway-Nodes, einen externen Load Balancer von 3^Rd^ Party, DNS-Round Robin oder direkt zu einem Storage-Node implementiert werden. Mehrere Gateway Nodes können an einem Standort implementiert und in Hochverfügbarkeitsgruppen konfiguriert werden, die für automatisches Failover und Failback bei einem Ausfall des Gateway Node sorgen. Sie können Lastausgleichsmethoden in einer Lösung kombinieren, um einen zentralen Zugriffspunkt für alle Standorte in einer Lösung bereitzustellen.

Die Gateway-Nodes gleichen die Last zwischen den Speicher-Nodes an dem Standort aus, an dem sich der Gateway-Node befindet. StorageGRID kann so konfiguriert werden, dass die Gateway-Nodes mithilfe von Nodes von mehreren Standorten eine Lastenverteilung erhalten. Bei dieser Konfiguration würde die Latenz zwischen diesen Standorten auf die Antwortlatenz für die Clientanfragen erhöht. Diese Einstellung sollte nur konfiguriert werden, wenn die Gesamtlatenz für die Clients akzeptabel ist.



== So erreichen Sie mit StorageGRID ein RPO von null

Um ein Recovery Point Objective (RPO) von null in einem Objekt-Storage-System zu erreichen, ist es bei einem Ausfall entscheidend:

* Sowohl Metadaten als auch Objektinhalte werden synchron betrachtet und als konsistent betrachtet
* Der Zugriff auf den Objektinhalt bleibt trotz des Fehlers erhalten.


Für eine Bereitstellung an mehreren Standorten ist Strong Global V2 das bevorzugte Konsistenzmodell, um sicherzustellen, dass Metadaten an allen Standorten synchronisiert werden. Dies macht es unerlässlich, die RPO-Anforderungen von Null zu erfüllen.

Objekte im Storage-System werden nach ILM-Regeln (Information Lifecycle Management) gespeichert, die festlegen, wie und wo Daten während ihres gesamten Lebenszyklus gespeichert werden. Bei der synchronen Replikation kann zwischen strenger Ausführung oder ausgeglichener Ausführung berücksichtigt werden.

* Für ein RPO von null ist eine strikte Ausführung dieser ILM-Regeln nötig, da so sichergestellt wird, dass Objekte ohne Verzögerung oder Fallback an den definierten Standorten platziert werden, sodass die Datenverfügbarkeit und -Konsistenz erhalten bleiben.
* Das ILM-Balance-Aufnahmeverhalten von StorageGRID sorgt für ein Gleichgewicht zwischen Hochverfügbarkeit und Ausfallsicherheit, sodass Benutzer auch bei einem Standortausfall weiterhin Daten aufnehmen können.


Optional kann mit einer Kombination aus lokalem und globalem Lastenausgleich eine RTO von null erreicht werden. Um einen unterbrechungsfreien Client-Zugriff zu gewährleisten, ist ein Lastausgleich für Client-Anfragen erforderlich. Eine StorageGRID-Lösung kann an jedem Standort mehrere Gateway-Nodes und Hochverfügbarkeitsgruppen enthalten. Um einen unterbrechungsfreien Zugriff für Clients an jedem Standort zu ermöglichen, selbst bei einem Standortausfall, sollten Sie eine externe Load Balancing-Lösung in Kombination mit den StorageGRID Gateway Nodes konfigurieren. Konfigurieren Sie die Hochverfügbarkeitsgruppen des Gateway-Node, die die Last innerhalb jedes Standorts verwalten, und verwenden Sie den externen Load Balancer, um die Last über die Hochverfügbarkeitsgruppen hinweg auszugleichen. Der externe Load Balancer muss so konfiguriert werden, dass eine Integritätsprüfung durchgeführt wird, um sicherzustellen, dass Anfragen nur an betriebsbereite Standorte gesendet werden. Weitere Informationen zum Lastausgleich mit StorageGRID finden Sie im https://www.netapp.com/media/17068-tr4626.pdf["Technischer Bericht zum StorageGRID Load Balancer"].



== Synchrone Implementierungen an mehreren Standorten

*Multi-Site-Lösungen:* mit StorageGRID können Sie Objekte synchron über mehrere Standorte innerhalb des Grids hinweg replizieren. Durch die Einrichtung von ILM-Regeln (Information Lifecycle Management) mit Balance oder striktem Verhalten werden Objekte sofort an den angegebenen Speicherorten platziert. Durch die Konfiguration der Bucket-Konsistenzstufe auf Strong Global v2 wird auch die synchrone Metadatenreplikation sichergestellt. StorageGRID verwendet einen einzelnen globalen Namespace und speichert Objektspeicher als Metadaten, sodass jeder Node weiß, an welchem Ort sich alle Kopien oder Elemente, die nach Erasure Coding geschrieben werden, befinden. Wenn ein Objekt nicht von dem Standort abgerufen werden kann, an dem die Anfrage gestellt wurde, wird es automatisch von einem Remote-Standort abgerufen, ohne dass ein Failover erforderlich ist.

Sobald der Ausfall behoben ist, sind keine manuellen Failback-Prozesse erforderlich. Die Replizierungs-Performance hängt von dem Standort mit dem niedrigsten Netzwerkdurchsatz, der höchsten Latenz und der niedrigsten Performance ab. Die Performance eines Standorts basiert auf der Anzahl der Nodes, der Anzahl und Geschwindigkeit der CPU-Kerne, dem Arbeitsspeicher, der Anzahl der Laufwerke und den Laufwerkstypen.

*Multi-Grid-Lösungen:* StorageGRID kann Mandanten, Benutzer und Buckets mithilfe von Grid-übergreifender Replikation (CGR) zwischen mehreren StorageGRID-Systemen replizieren. CGR kann ausgewählte Daten auf mehr als 16 Standorte erweitern, die nutzbare Kapazität Ihres Objektspeichers erhöhen und Disaster Recovery bereitstellen. Die Replikation von Buckets mit CGR umfasst Objekte, Objektversionen und Metadaten und kann bidirektional oder einseitig erfolgen. Der Recovery-Zeitpunkt (Recovery Point Objective, RPO) hängt von der Performance des jeweiligen StorageGRID-Systems und der Netzwerkverbindungen zwischen diesen Systemen ab.

*Zusammenfassung:*

* Die Grid-interne Replizierung umfasst sowohl synchrone als auch asynchrone Replizierung, die mithilfe des ILM-Aufnahmeverhaltens und der Konsistenzkontrolle für Metadaten konfigurierbar ist.
* Die Replizierung zwischen dem Grid erfolgt nur asynchron.




== Bereitstellung über mehrere Standorte in einem einzigen Grid

In den folgenden Szenarien werden die StorageGRID-Lösungen mit einem optionalen externen Load Balancer konfiguriert, der Anfragen an die integrierten Load Balancer-Hochverfügbarkeitsgruppen verwaltet. Somit wird neben dem RPO von Null ein RTO von null erreicht. ILM verfügt über ausgewogenen Aufnahmeschutz für die synchrone Platzierung. Jeder Bucket ist mit dem starken globalen v2-Konsistenzmodell für Grids von 3 oder mehr Standorten und mit starker globaler Konsistenz für weniger als 3 Standorte konfiguriert.

In einer StorageGRID-Lösung mit zwei Standorten gibt es mindestens zwei Replikate oder 3 EC-Blöcke jedes Objekts und 6 Replikate aller Metadaten. Bei der Wiederherstellung werden die Updates nach dem Ausfall automatisch mit dem wiederhergestellten Standort/den wiederhergestellten Nodes synchronisiert. Bei nur 2 Standorten wird es wahrscheinlich nicht möglich sein, ein RPO von null in Ausfallszenarien zu erzielen, die über einen vollständigen Standortausfall hinausgehen.

image:zero-rpo/2-site.png["StorageGRID System mit zwei Standorten"]

In einer StorageGRID-Lösung mit drei oder mehr Standorten gibt es mindestens 3 Replikate oder 3 EC-Blöcke jedes Objekts und 9 Replikate aller Metadaten. Bei der Wiederherstellung werden die Updates nach dem Ausfall automatisch mit dem wiederhergestellten Standort/den wiederhergestellten Nodes synchronisiert. Mit drei oder mehr Standorten wird ein RPO von Null erreicht.

image:zero-rpo/3-site.png["StorageGRID System mit drei Standorten"]

Ausfallszenarien für mehrere Standorte

[cols="34%,33%,33%"]
|===
| Ausfall | 2-Site-Ergebnis | 3 oder mehr Websites Ergebnis 


| Ausfall eines Laufwerks mit einem Node | Jede Appliance nutzt mehrere Festplattengruppen und kann den Ausfall von mindestens einem Laufwerk pro Gruppe ohne Unterbrechung oder Datenverlust überstehen. | Jede Appliance nutzt mehrere Festplattengruppen und kann den Ausfall von mindestens einem Laufwerk pro Gruppe ohne Unterbrechung oder Datenverlust überstehen. 


| Ausfall eines einzelnen Nodes an einem Standort | Keine Unterbrechung von Prozessen oder Datenverlust: | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall mehrerer Nodes an einem Standort  a| 
Auf diesen Standort gerichtete Unterbrechung von Client-Vorgängen, jedoch kein Datenverlust.

Der auf den anderen Standort gerichtete Betrieb bleibt ohne Unterbrechung und ohne Datenverlust erhalten.
| Der Betrieb wird auf alle anderen Standorte geleitet und erfolgt ohne Unterbrechung und Datenverlust. 


| Ausfall eines einzelnen Nodes an mehreren Standorten  a| 
Keine Unterbrechungen oder Datenverluste bei:

* Im Raster ist mindestens ein einzelnes Replikat vorhanden
* Im Raster sind ausreichend EC-Blöcke vorhanden


Betriebsausfall und Gefahr von Datenverlusten bei:

* Es sind keine Replikate vorhanden
* Es sind nicht genügend EC-Spannfutter vorhanden

 a| 
Keine Unterbrechungen oder Datenverluste bei:

* Im Raster ist mindestens ein einzelnes Replikat vorhanden
* Im Raster sind ausreichend EC-Blöcke vorhanden


Betriebsausfall und Gefahr von Datenverlusten bei:

* Es sind keine Replikate vorhanden
* Es sind nicht genügend EC-Chucks vorhanden, um das Objekt abzurufen




| Ausfall eines einzelnen Standorts | Der Client-Betrieb wird unterbrochen, bis entweder der Fehler behoben oder die Bucket-Konsistenz auf einen starken Standort oder niedriger gesenkt wird, damit der Betrieb erfolgreich ausgeführt werden kann, aber kein Datenverlust auftritt. | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall eines Standorts und eines einzelnen Node | Der Client-Betrieb wird unterbrochen, bis entweder der Fehler behoben oder die Bucket-Konsistenz auf Read-after-New-Write oder niedriger gesenkt wird, um einen erfolgreichen Betrieb und möglichen Datenverlust zu ermöglichen. | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Von jedem verbleibenden Standort aus einen Standort und einen Node | Der Client-Betrieb wird unterbrochen, bis entweder der Fehler behoben oder die Bucket-Konsistenz auf Read-after-New-Write oder niedriger gesenkt wird, um einen erfolgreichen Betrieb und möglichen Datenverlust zu ermöglichen. | Der Betrieb wird unterbrochen, wenn das Quorum der Metadatenreplikate nicht erfüllt werden kann und ein möglicher Datenverlust möglich ist. 


| Ausfall mehrerer Standorte | Keine Betriebsstandorte bleiben Daten verloren gehen, wenn mindestens ein Standort nicht vollständig wiederhergestellt werden kann. | Der Betrieb wird unterbrochen, wenn das Quorum der Metadatenreplikate nicht erfüllt werden kann. Kein Datenverlust, solange mindestens ein Standort verbleibt. 


| Netzwerkisolierung eines Standorts | Der Client-Betrieb wird unterbrochen, bis entweder der Fehler behoben oder die Bucket-Konsistenz auf einen starken Standort oder niedriger gesenkt wird, um den Betrieb erfolgreich zu machen, aber keinen Datenverlust  a| 
Der Betrieb des isolierten Standorts wird unterbrochen, es kommt jedoch zu keinem Datenverlust

Es gibt keine Unterbrechung des Betriebs an den verbleibenden Standorten und keinen Datenverlust

|===


== Eine Multi-Grid-Implementierung an mehreren Standorten

Um eine zusätzliche Redundanzebene hinzuzufügen, verwendet dieses Szenario zwei StorageGRID-Cluster und nutzt die Grid-übergreifende Replizierung, um sie synchron zu halten. Für diese Lösung verfügt jeder StorageGRID-Cluster über drei Standorte. Zwei Standorte werden für Objekt-Storage und Metadaten verwendet, der dritte Standort lediglich für Metadaten. Beide Systeme werden mit einer ausgewogenen ILM-Regel konfiguriert, um die Objekte mittels Erasure Coding in jedem der beiden Datenstandorte synchron zu speichern. Buckets werden mit dem starken globalen v2-Konsistenzmodell konfiguriert. Jedes Grid wird für jeden Bucket mit bidirektionaler Grid-Replizierung konfiguriert. Dadurch wird die asynchrone Replikation zwischen den Regionen bereitgestellt. Optional kann ein globaler Load Balancer zum Managen von Anfragen an die integrierten Load Balancer mit Hochverfügbarkeitsgruppen beider StorageGRID Systeme implementiert werden, um ein RPO von null zu erzielen.

Die Lösung nutzt vier Standorte, die gleichmäßig in zwei Regionen aufgeteilt sind. Region 1 enthält die 2 Storage-Standorte von Grid 1 als primäres Grid der Region und den Metadaten-Standort von Grid 2. Region 2 enthält die 2 Storage-Standorte von Grid 2 als primäres Grid der Region und den Metadaten-Standort von Grid 1. In jeder Region kann der gleiche Standort den Speicherort des primären Grids der Region sowie den nur-Metadaten-Standort des anderen Regionengitters beherbergen. Wenn Nodes als dritter Standort nur Metadaten verwendet werden, sorgen sie für die erforderliche Konsistenz für die Metadaten und nicht für das Duplizieren des Storage von Objekten an diesem Standort.

image:zero-rpo/2x-grid-3-site.png["Die Multi-Grid-Lösung mit vier Standorten"]

Diese Lösung mit vier separaten Standorten bietet vollständige Redundanz von zwei separaten StorageGRID-Systemen mit einem RPO von 0 und nutzt sowohl synchrone Replizierung an mehreren Standorten als auch asynchrone Replizierung in mehreren Grids. Bei jedem einzelnen Standort kann der Client-Betrieb auf beiden StorageGRID Systemen unterbrechungsfrei ausgeführt werden.

In dieser Lösung gibt es vier Kopien, die nach Erasure Coding codiert wurden, und 18 Replikate aller Metadaten. Dies ermöglicht mehrere Ausfallszenarien ohne Auswirkungen auf den Client-Betrieb. Bei einem Ausfall werden die Updates nach dem Ausfall automatisch mit dem ausgefallenen Standort bzw. den ausgefallenen Nodes synchronisiert.

Ausfallszenarien für mehrere Standorte und Grids

[cols="50%,50%"]
|===
| Ausfall | Ergebnis 


| Ausfall eines Laufwerks mit einem Node | Jede Appliance nutzt mehrere Festplattengruppen und kann den Ausfall von mindestens einem Laufwerk pro Gruppe ohne Unterbrechung oder Datenverlust überstehen. 


| Ausfall eines einzelnen Nodes an einem Standort in einem Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall eines einzelnen Nodes an einem Standort in jedem Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall mehrerer Nodes an einem Standort in einem Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall mehrerer Nodes an einem Standort in jedem Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall eines einzelnen Nodes an mehreren Standorten in einem Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall eines einzelnen Nodes an mehreren Standorten in jedem Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


|  |  


| Ausfall eines einzelnen Standorts in einem Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall eines Standorts in jedem Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall eines einzelnen Standorts und eines einzelnen Node in einem Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ein Standort und ein Node von jedem verbleibenden Standort in einem einzelnen Grid | Keine Unterbrechung von Prozessen oder Datenverlust: 


|  |  


| Ausfall eines einzelnen Standorts | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall eines Standorts in jedem Grid DC1 und DC3  a| 
Der Betrieb wird unterbrochen, bis entweder der Fehler behoben oder die Bucket-Konsistenz verringert wird; jedes Grid hat 2 Standorte verloren

Alle Daten sind noch an 2 Standorten vorhanden



| Ausfall eines Standorts in jedem Grid DC1 und DC4 oder DC2 und DC3 | Keine Unterbrechung von Prozessen oder Datenverlust: 


| Ausfall eines Standorts in jedem Grid DC2 und DC4 | Keine Unterbrechung von Prozessen oder Datenverlust: 


|  |  


| Netzwerkisolierung eines Standorts  a| 
Der Betrieb des isolierten Standorts wird unterbrochen, aber es gehen keine Daten verloren

Es gibt keine Unterbrechung des Betriebs an den verbleibenden Standorten oder Datenverluste.

|===


== Schlussfolgerung

Das Erreichen eines Recovery Point Objective (RPO) von null mit StorageGRID ist ein wichtiges Ziel, um die Datenaufbewahrung und Verfügbarkeit bei Standortausfällen sicherzustellen. Durch den Einsatz der robusten Replikationsstrategien von StorageGRID, einschließlich synchroner Replizierung an mehreren Standorten und asynchroner Multi-Grid-Replizierung, können Unternehmen den unterbrechungsfreien Client-Betrieb gewährleisten und über mehrere Standorte hinweg für Datenkonsistenz sorgen. Die Implementierung von ILM-Richtlinien (Information Lifecycle Management) und die Verwendung von Nodes, die nur Metadaten enthalten, erhöhen die Ausfallsicherheit und Performance des Systems noch weiter. Mit StorageGRID können Unternehmen ihre Daten zuversichtlich managen, da sie wissen, dass sie auch bei komplexen Ausfallszenarien zugänglich und konsistent bleiben. Dieser umfassende Ansatz für Datenmanagement und -Replikation unterstreicht die Bedeutung einer sorgfältigen Planung und Ausführung bei der Erreichung eines Null-RPO-Ziels und der Sicherung wertvoller Informationen.
